{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Author: Alastair Hamilton\n",
    "# Date: May/June 2018\n",
    "# Title: Model for Home-depot Kaggle Competition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Data Wrangling\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Misc\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint as pp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## NLP\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import *\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Set path to data\n",
    "data_path = \"../.data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Processing features\n",
    "proc_feat = ['search_term', 'product_title', 'product_description', 'attributes']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Tokenise a pandas Series\n",
    "def tokenise(s, tokeniser, tokenise_fn=False):\n",
    "    if tokenise_fn:\n",
    "        return s.apply(tokeniser.tokenize)\n",
    "    else:\n",
    "        return s.apply(tokeniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def stem(s, stemmer):\n",
    "    return s.apply(lambda x: tuple(map(stemmer.stem, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove punctuation from a pandas Series\n",
    "def rmv_punc(s):\n",
    "    return s.apply(lambda x: tuple(filter(lambda y: not y.is_punct, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def rmv_stop(s, stops):\n",
    "    return s.apply(lambda x: tuple(filter(lambda y: y not in stops, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Apply function on rows of data frame (2 cols max)\n",
    "def func_row(df, func):\n",
    "    return df.apply(lambda row: func(row[0], row[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Find number of words in one document (doc1) that are in another document (doc2)\n",
    "def common_words_doc(doc1, doc2):\n",
    "    tot = 0\n",
    "    for w1 in doc1:\n",
    "        for w2 in doc2:\n",
    "            if w2.find(w1) >= 0:\n",
    "                tot += 1\n",
    "                break\n",
    "    return tot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "- Importing attributes.csv.zip...\n",
      "- Importing test.csv.zip...\n",
      "- Importing train.csv.zip...\n",
      "- Importing product_descriptions.csv.zip...\n",
      "- Importing sample_submission.csv.zip...\n"
     ]
    }
   ],
   "source": [
    "print('Importing data...')\n",
    "\n",
    "# # Get all zipped files in data path\n",
    "zips = [f for f in os.listdir(data_path) if re.search(\".zip$\", f)]\n",
    "\n",
    "# # Unzip all files and put into dictionary, keyed by file stem\n",
    "data_dict = {}\n",
    "for zipped in zips:\n",
    "    print('- Importing {}...'.format(zipped))\n",
    "    if zipped == 'attributes.csv.zip':\n",
    "        encoding = 'utf-8'\n",
    "    else:\n",
    "        encoding = 'latin1'\n",
    "    data_dict[zipped.split('.')[0]] = pd.read_csv(data_path+zipped, compression='zip', encoding=encoding)\n",
    "\n",
    "# # Set dataframe to piece in data dictionary\n",
    "train_df = data_dict['train']\n",
    "prod_desc = data_dict['product_descriptions']\n",
    "attributes = data_dict['attributes']\n",
    "\n",
    "# # Clean up\n",
    "del data_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "- Handling attributes data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing data...\")\n",
    "\n",
    "# # Process attributes data\n",
    "print(\"- Handling attributes data...\")\n",
    "\n",
    "# # # Deal with N/As in attributes data\n",
    "attr = attributes.dropna(how='all')\n",
    "attr = attributes.dropna(how='all', subset=['value'])\n",
    "\n",
    "# # # Ensure UID is int\n",
    "attr.loc[:, 'product_uid'] = attr['product_uid'].apply(lambda x: int(x))\n",
    "\n",
    "# # # If starts with Bullet followed by 2 digits then a bullet point\n",
    "# # # If bullet then replace with '*'\n",
    "def is_match(pattern, string):\n",
    "    return bool(re.match(pattern, string))\n",
    "bullet_point_regex_pattern = \"Bullet\\\\d{2}\"\n",
    "attr.loc[:, 'name'] = attr['name'].apply(lambda s: '*' if is_match(bullet_point_regex_pattern, s) else s)\n",
    "\n",
    "# # # Group name and value in attributes into single column, marking name and value\n",
    "attr = attr.assign(\n",
    "    attributes =\n",
    "        attr.apply(\n",
    "            lambda row:\n",
    "                    row['name'] + ' ' + row['value'] if row['name'] == '*'\n",
    "                    else row['name'] + ':' + row['value'],\n",
    "            axis=1\n",
    "        )\n",
    ")\n",
    "\n",
    "# # # Drop name and values, groupby UID and sum grouped values, reset index...\n",
    "# # # ...(ie. all attributes in single cell now)\n",
    "attr = attr.drop(['name','value'], axis=1).groupby('product_uid').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Creating main dataframe...\n"
     ]
    }
   ],
   "source": [
    "# # Create master data frame\n",
    "print(\"- Creating main dataframe...\")\n",
    "\n",
    "# # # Merge all data into one master dataframe by merging descriptions and attributes onto training data on UID...\n",
    "# # # ...Fill any NAs with empty string\n",
    "data = pd.merge(train_df, prod_desc, how='left',\n",
    "                on='product_uid').drop('id', axis=1).merge(attr, on='product_uid', how='left').fillna('')\n",
    "\n",
    "# # # Finally create a master index column, which will be used to reference individual search terms\n",
    "data = data.reset_index(drop=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cols = ['product_title', 'search_term', 'product_description', 'attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = data.copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation\n",
    "Using the regexp tokeniser in NLTK with ```r'\\w+'``` was significantly faster and got rid of punctuation, which was intended.\n",
    "\n",
    "- Need to add it to ignore the weird a^ character\n",
    "\n",
    "|Tokeniser|Time taken/200 cells in product title (ms)|\n",
    "|---|---|\n",
    "|Regexp|1.28|\n",
    "|wordpunct|1.57|\n",
    "|wordtokenise|48.5|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenise_anon = lambda x: tokenise(x, RegexpTokenizer(r'\\w+'), tokenise_fn=True)\n",
    "data_proc.loc[:, text_cols] = data_proc.loc[:, text_cols].apply(tokenise_anon, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Stop Words\n",
    "Caching the stop words corpus was ridiculously faster (ie loading ```stopwords.words('english')``` once).\n",
    "\n",
    "|Remover|Time taken/200 cells in product title (ms)|\n",
    "|---|---|\n",
    "|nltk stop corpus filter (no cache)|435|\n",
    "|nltk stop corpus filter (cache)|6.71|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_anon = lambda x: rmv_stop(x, stopwords.words('english'))\n",
    "data_proc.loc[:, text_cols] = data_proc.loc[:, text_cols].apply(stopwords_anon, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Worth seeing if this affects the model further down the line. \n",
    "- Choosing snowball as faster and seen it be used for this problem before [9/5/19]\n",
    "\n",
    "|Stemmer|Time taken/200 cells in product title (ms)|\n",
    "|---|---|\n",
    "|Porter|45|\n",
    "|Snowball|37.9|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_anon = lambda x: stem(x, SnowballStemmer('english'))\n",
    "data_proc.loc[:, text_cols] = data_proc.loc[:, text_cols].apply(stemming_anon, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_uid            0\n",
       "product_title          0\n",
       "search_term            0\n",
       "relevance              0\n",
       "product_description    0\n",
       "attributes             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proc.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>relevance</th>\n",
       "      <th>product_description</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>(simpson, strong, tie, 12, gaug, angl)</td>\n",
       "      <td>(angl, bracket)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(not, angl, make, joint, stronger, also, provi...</td>\n",
       "      <td>(versatil, connector, various, 90, connect, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>(simpson, strong, tie, 12, gaug, angl)</td>\n",
       "      <td>(l, bracket)</td>\n",
       "      <td>2.5</td>\n",
       "      <td>(not, angl, make, joint, stronger, also, provi...</td>\n",
       "      <td>(versatil, connector, various, 90, connect, ho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid                           product_title      search_term  \\\n",
       "0       100001  (simpson, strong, tie, 12, gaug, angl)  (angl, bracket)   \n",
       "1       100001  (simpson, strong, tie, 12, gaug, angl)     (l, bracket)   \n",
       "\n",
       "   relevance                                product_description  \\\n",
       "0        3.0  (not, angl, make, joint, stronger, also, provi...   \n",
       "1        2.5  (not, angl, make, joint, stronger, also, provi...   \n",
       "\n",
       "                                          attributes  \n",
       "0  (versatil, connector, various, 90, connect, ho...  \n",
       "1  (versatil, connector, various, 90, connect, ho...  "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feat = pd.DataFrame(data_proc[['product_uid', 'relevance']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Len of query\n",
    "data_feat['q_len'] = data_proc['search_term'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product title\n",
    "data_feat['com_title'] = func_row(data_proc[['search_term', 'product_title']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned product description\n",
    "data_feat['com_desc'] = func_row(data_proc[['search_term', 'product_description']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get common words between query and returned attributes\n",
    "data_feat['com_attr'] = func_row(data_proc[['search_term', 'attributes']], common_words_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_uid</th>\n",
       "      <th>relevance</th>\n",
       "      <th>q_len</th>\n",
       "      <th>com_title</th>\n",
       "      <th>com_desc</th>\n",
       "      <th>com_attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100001</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_uid  relevance  q_len  com_title  com_desc  com_attr\n",
       "0       100001        3.0      2          1         1         1\n",
       "1       100001        2.5      2          1         1         1"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_feat.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "home-depot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "notify_time": "10",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
